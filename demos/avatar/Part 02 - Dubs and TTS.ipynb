{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fb162d-185c-4014-9914-7b5c6ff684e3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this demo, we will show how dubbing works.\n",
    "\n",
    "The main data classes are located in `kaia.persona.dub.core.structures`. \n",
    "\n",
    "* `Dub` is an abstract class representing the connection of some value (of arbitrary type) to the string and voice line.\n",
    "* `SetDub` is a class that binds values of a finite set. `DictDub` and `EnumDub` are its descendants for dictionaries and enums.\n",
    "* `SequenceDub` is a sequence of constants and some other dubs.\n",
    "* `UnionDub` contains several sequences. The idea is that it gets the values, converts it to dict, then finds a sequence processing these values, and uses this sequence to create a string representation of the value.\n",
    "\n",
    "Other dubs are language-specific and are located in `kaia.persona.dub.languages.en`. These are, e.g., `CardinalDub` and `OrdinalDub` which inherit `SetDub` and represent numbers; or `DateDub` which extends `UnionDub` and processes `datetime.date` objects.\n",
    "\n",
    "To define intents and replies of the assistant, `Template` class is used; this class contains `UnionDub` as a field. `Template` also contains methods for parsing, to-string convertion and others. These methods are shortcuts for algorithms that are located in `kaia.persona.dub.core.algorithms`. These algorithms are implementations of depth-first search over `UnionDub`, and you don't need to import them directly.\n",
    "\n",
    "To represent a particular sentence that is a combination of `Template` and the associated value, `Utterance` is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce815d-339a-4f42-92e5-09ad39c1d64b",
   "metadata": {},
   "source": [
    "## Templates\n",
    "\n",
    "We will now create a template of an average complexity to demonstrate how dubbing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d15f474-e449-43e7-9cf7-3c975164c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaia.avatar.dub.languages.en import Template, CardinalDub, PluralAgreement\n",
    "\n",
    "template = Template(\n",
    "    'It is {hours} {hours_word} and {minutes} {minutes_word}',\n",
    "    hours = CardinalDub(0, 24),\n",
    "    hours_word = PluralAgreement('hours', 'hour', 'hours'),\n",
    "    minutes = CardinalDub(0, 60),\n",
    "    minutes_word = PluralAgreement('minutes', 'minute', 'minutes')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70f83ab-78ff-4ce0-b613-7c67e39d5654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is eleven hours and one minute'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = dict(hours=11, minutes=1)\n",
    "string = template.to_str(value)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3398a6c-9c1a-47fb-a94b-ef6647337c95",
   "metadata": {},
   "source": [
    "Notice the word \"hours\" and \"minute\". The form is choosen by `PluralAgreement` in accordance with the value of the corresponding field.\n",
    "\n",
    "Template can also parse strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cddfb61a-4cb6-4a3d-bbcb-3677f7458d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minutes': 1, 'hours': 11}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.parse(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ecefb-9c55-43ad-9540-311b11f8038d",
   "metadata": {},
   "source": [
    "Templates can be defined within `TemplatesCollection` subclasses for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85243403-cc98-46a9-be81-c1fa0d048cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__.MyTemplates.hello'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.avatar.dub.languages.en import TemplatesCollection\n",
    "\n",
    "class MyTemplates(TemplatesCollection):\n",
    "    hello = Template(\"Hello\")\n",
    "    how_are_you = Template(\"How are you?\")\n",
    "\n",
    "MyTemplates.hello.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9d280-3098-43cc-b7fc-b37bd1398f59",
   "metadata": {},
   "source": [
    "That will assign the name of the field to the template.\n",
    "\n",
    "We have defined a big class with lots of intents for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce5faaa-542c-4a80-8881-9c56d4927e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaia.avatar.dub.sandbox.intents.Intents.yes',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.no',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.time',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.date',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.weather',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.transport',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.timer_create',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.timer_how_much_time',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.timer_how_many_timers',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.timer_cancel',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.spotify',\n",
       " 'kaia.avatar.dub.sandbox.intents.Intents.cook']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.avatar.dub.sandbox import Intents\n",
    "\n",
    "[i.name for i in Intents.get_templates()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14657f9c-43a7-4eb9-85dc-4166c76099c7",
   "metadata": {},
   "source": [
    "## Voiceover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb1482-3e00-4f55-9c16-f62666188f98",
   "metadata": {},
   "source": [
    "Voiceover is performed by BrainBox. Currently, there are two solutions for this:\n",
    "\n",
    "* [https://github.com/synesthesiam/opentts](OpenTTS): a ready container that can do text-to-speech for many languages through various techniques. While working out of the box, it does not provide the voice cloning.\n",
    "* CoquiTTS container that hosts available models (YourTTS, VITS and XTTS), as well as the fine-tuning results.\n",
    "\n",
    "While TortoiseTTS is also doing text-to-speech, it's unpractical for this purpose, as it's slow and requires GPU. OpenTTS and CoquiTTS do not require GPU and use VITS-based models that works very fast. \n",
    "\n",
    "Other models are sure possible. Voiceover should be implemented as a BrainBoxDecider that takes text and voice, and returns the list of names (probably only one) of the audio files created. This file can the be downloaded and used for voiceover. \n",
    "\n",
    "The following code demonstrates the protocol. For the remaining of the demo, the following requirements apply:\n",
    "\n",
    "* docker must be enabled and configured.\n",
    "* OpenTTS demo from `brainbox_deciders` should be completed.\n",
    "\n",
    "To run voiceover, we need a function that converts `text` and `voice` variables into BrainBoxTaskPack. In our case, it is going to be the single request to OpenTTS decider.\n",
    "\n",
    "The result of the following cell should apper within few seconds even without GPU. If it's not the case, check BrainBox output console: it's probable that OpenTTS wasn't run in the `brainbox_decider` section and the container is being pulled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f036bb85-6290-4b20-9a63-d8d56cdd97a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee2e9d1d6034ae88dabc1eacc39c8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Audio(value=b'RIFFD\\x0c\\x01\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\"V\\x00\\x00D\\xac\\x00\\x00\\x02\\x00\\x10\\x00â€¦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.brainbox import BrainBox, BrainBoxTask, BrainBoxTaskPack, DownloadingPostprocessor\n",
    "from kaia.infra import FileIO\n",
    "from ipywidgets import Audio\n",
    "from kaia.brainbox import BrainBox\n",
    "\n",
    "def task_generator(text, voice):\n",
    "    return BrainBoxTaskPack(\n",
    "        BrainBoxTask(\n",
    "            id = BrainBoxTask.safe_id(), \n",
    "            decider='OpenTTS', \n",
    "            arguments=dict(voice='coqui-tts:en_vctk', lang='en', speakerId=voice, text=text)\n",
    "            ),\n",
    "        (),\n",
    "        DownloadingPostprocessor(take_element_before_downloading=0, opener=FileIO.read_bytes)\n",
    "    )\n",
    "            \n",
    "pack = task_generator('Hello world', 'p225')\n",
    "with BrainBox().create_test_api() as api:\n",
    "    result = api.execute(pack)\n",
    "Audio(value=result, autoplay = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950d6bb-a21f-4aed-adc7-93890b958d85",
   "metadata": {},
   "source": [
    "Now, `DubbingService` does pretty much the same: it executes the tasks, and also handles `Utterances` and caching: if something was already spoken, it won't be generated for the second time. Also, cache can be used for long outputs: separate it into several strings with `preview` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f16533-82bd-4fff-a738-cd04b7fba508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e350f01792994d93b11adff3ad09c4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Audio(value=b'RIFFD\\xd0\\x01\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\"V\\x00\\x00D\\xac\\x00\\x00\\x02\\x00\\x10\\x00â€¦"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaia.avatar.server.dubbing_service import BrainBoxDubbingService\n",
    "\n",
    "with BrainBox().create_test_api() as api:\n",
    "    service = BrainBoxDubbingService(task_generator, api)\n",
    "    audio = service.dub_string('Hello world via Dubbing Service', 'p225')\n",
    "Audio(value = audio.data, autoplay = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb1eb4-e6b3-42bc-a28f-f4792b7d5fed",
   "metadata": {},
   "source": [
    "`dub_string` returns `kaia.eaglesong.Audio` object, which is handy to use in voice assistant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
